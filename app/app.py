import sys, os
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import streamlit as st
from dotenv import load_dotenv

# ëª¨ë“ˆ ì„í¬íŠ¸
from vectorDB.document_loader import load_documents
from vectorDB.text_splitter import split_documents
from embedding.embeddings import embedding
from vectorDB.chunker import semantic_chunker
from vectorDB.vectorstore import vectorstore, vectorstore_semantic
from retriever.retriever import retriever
from model.chain import create_models

from langchain_core.prompts import PromptTemplate
from langchain_upstage import UpstageGroundednessCheck
from langchain_community.retrievers import TavilySearchAPIRetriever


def initialize_session():
    if "rag_state" not in st.session_state:
        st.session_state.rag_state = False


def setup_page():
    st.set_page_config(layout="wide")
    st.title("LangChain Project")


def sidebar_settings():
    st.subheader("Settings")
    st.markdown("---")
    st.session_state.chunk_size = st.number_input("chunk_size", value=1000, step=50)
    st.session_state.chunk_overlap = st.number_input("chunk_overlap", value=250, step=50)
    st.session_state.method = st.radio("ë‹µë³€ ìƒì„± ë°©ì‹ ì„ íƒ", ("ì„ë² ë”© ê¸°ë°˜", "ì‹œë©˜í‹± ì²­ì»¤ ê¸°ë°˜"))

    default_prompt = (
        "ë„ˆëŠ” ìŠ¤í¬ì¸  ê·œì¹™ ì „ë¬¸ê°€ì•¼.\n"
        "ë„ˆëŠ” ê²½ê¸° ê·œì¹™ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜ì•¼í•´.\n"
        "ë„ˆëŠ” ë‹µë³€ì„ í•  ë•Œ, ë°˜ë“œì‹œ ì¢…ëª©ì„ í™•ì¸í•´ì„œ ë‹µë³€í•´ì¤˜ì•¼í•´.\n"
        "ë„ˆëŠ” ë‹µë³€ì„ í•  ë•Œ, ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜ì•¼í•´.\n"
        "1.\n2.\n3.\n\n"
        "#Question: \n{question} \n#Context: \n{context} \n\n#Answer:"
    )
    st.session_state.new_prompt = st.text_area("í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”!", value=default_prompt, height=350, key="prompt_write")
    if st.button("ì‹¤í–‰"):
        st.session_state.rag_state = True


def run_pipeline():
    st.session_state.pdf_files, st.session_state.all_docs = load_documents()
    st.session_state.split_documents = split_documents(
        all_docs=st.session_state.all_docs,
        chunk_size=st.session_state.chunk_size,
        chunk_overlap=st.session_state.chunk_overlap,
    )
    st.session_state.embeddings = embedding(openai_api_key=st.session_state.openai_api_key)

    if st.session_state.method == "ì„ë² ë”© ê¸°ë°˜":
        st.session_state.vectorstore = vectorstore(st.session_state.embeddings, st.session_state.split_documents)
        st.session_state.retriever = retriever(st.session_state.vectorstore)
    else:  # ì‹œë©˜í‹± ì²­ì»¤ ê¸°ë°˜
        if "semantic_chunks" not in st.session_state:
            st.session_state.semantic_chunks = semantic_chunker(
                st.session_state.embeddings,
                st.session_state.split_documents,
                st.session_state.chunk_size,
                st.session_state.chunk_overlap,
            )
        st.session_state.vector_store_instance = vectorstore_semantic(
            st.session_state.embeddings, st.session_state.semantic_chunks
        )
        st.session_state.retriever = retriever(st.session_state.vector_store_instance)

    st.session_state.prompt = PromptTemplate.from_template(st.session_state.new_prompt)
    st.session_state.chain = create_models(
        st.session_state.retriever,
        st.session_state.prompt,
        st.session_state.openai_api_key,
    )


def render_result():
    st.subheader("Result")
    st.markdown("---")
    st.write(f"â¡ï¸ ì´ {len(st.session_state.pdf_files)}ê°œì˜ PDF íŒŒì¼ì—ì„œ {len(st.session_state.all_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    st.write(f"â¡ï¸ ë¶„í• ëœ ì²­í¬ì˜ ìˆ˜: {len(st.session_state.split_documents)}")
    if st.session_state.method == "ì‹œë©˜í‹± ì²­ì»¤ ê¸°ë°˜":
        st.write(f"â¡ï¸ ì‹œë©˜í‹± ì²­í¬ì˜ ìˆ˜: {len(st.session_state.semantic_chunks)}")
    st.text_area("â¡ï¸ í”„ë¡¬í”„íŠ¸ ë‚´ìš©", st.session_state.prompt.template, height=350)


def render_qa():
    st.subheader("QA")
    st.markdown("---")
    st.session_state.question = st.text_area("ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!", "", height=200)
    if st.button("ì‹¤í–‰", key="execute"):
        response = st.session_state.chain.invoke(st.session_state.question)
        st.markdown("---")
        st.subheader("Groundedness Check")

        # ê´€ë ¨ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        relevant_docs = st.session_state.retriever.get_relevant_documents(st.session_state.question)
        pdf_contents = "\n\n".join([doc.page_content for doc in relevant_docs])

        request_input = {"context": pdf_contents, "answer": response}
        st.session_state.groundedness_check = UpstageGroundednessCheck(
            api_key=st.session_state.openai_api_key,
            model="solar-pro",
            temperature=0,
            base_url="https://api.upstage.ai/v1",
        )
        st.session_state.gc_result = st.session_state.groundedness_check.invoke(request_input)

        if st.session_state.gc_result.lower().startswith("grounded"):
            st.text("âœ… Groundedness check passed")
            st.markdown(response)
        else:
            st.text("âŒ Groundedness check failed")
            st.text("ğŸ” Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
            st.session_state.tavily_retriever = TavilySearchAPIRetriever(k=3)
            tavily_chain = create_models(
                st.session_state.tavily_retriever,
                st.session_state.prompt,
                st.session_state.openai_api_key,
            )
            tavily_response = tavily_chain.invoke(st.session_state.question)
            st.markdown(tavily_response)


def main():
    load_dotenv()
    st.session_state.openai_api_key = os.getenv("OPENAI_API_KEY")
    st.session_state.tavily_api_key = os.getenv("TAVILY_API_KEY")
    
    setup_page()
    col1, col2, col3 = st.columns([1, 1, 2], border=True)

    with col1:
            sidebar_settings()

    if st.session_state.get("rag_state"):
        
        with col1:
            run_pipeline()
        with col2:
            render_result()
        with col3:
            render_qa()


if __name__ == "__main__":
    initialize_session()
    main()
