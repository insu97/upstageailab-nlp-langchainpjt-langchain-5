import os
import streamlit as st
from dotenv import load_dotenv

# ëª¨ë“ˆ ì„í¬íŠ¸
from function.pre_process import (
    load_documents,
    split_documents,
    embedding,
    sementic_chunker,
    vectorstore,
    vectorstore_sementic,
    retriever,
    create_models,
)

from langchain_core.prompts import PromptTemplate
from langchain_upstage import UpstageGroundednessCheck
from langchain_community.retrievers import TavilySearchAPIRetriever


if __name__ == "__main__":

    load_dotenv()
    st.session_state.openai_api_key = os.getenv("OPENAI_API_KEY")
    st.session_state.tavily_api_key = os.getenv("TAVILY_API_KEY")

    st.set_page_config(layout="wide")
    st.title("LangChain Project")

    col1, col2, col3 = st.columns([1, 1, 2], border=True)

    with col1:

        st.subheader("Settings")

        st.markdown("---")

        st.session_state.chunk_size = st.number_input("chunk_size", value=1000, step=50)
        st.session_state.chunk_overlap = st.number_input("chunk_overlap", value=250, step=50)

        # ë¼ë””ì˜¤ ë²„íŠ¼ìœ¼ë¡œ ë‹µë³€ ìƒì„± ë°©ì‹ì„ ì„ íƒí•©ë‹ˆë‹¤.
        st.session_state.method = st.radio("ë‹µë³€ ìƒì„± ë°©ì‹ ì„ íƒ", ("ì„ë² ë”© ê¸°ë°˜", "ì‹œë©˜í‹± ì²­ì»¤ ê¸°ë°˜"))

        prompt_text = """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Answer in Korean.

#Question: 
{question} 
#Context: 
{context} 

#Answer:"""
        
        st.session_state.new_prompt = st.text_area("í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”!", value=prompt_text, height=350, key="prompt_write")

        if "rag_state" not in st.session_state:
            st.session_state.rag_state = False

        if st.button("ì‹¤í–‰"):
            st.session_state.rag_state = True

    with col2:

        st.subheader("Result")

        st.markdown("---")

        if st.session_state.rag_state:
            st.session_state.pdf_files, st.session_state.all_docs = load_documents()
            st.session_state.split_documents = split_documents(all_docs=st.session_state.all_docs, chunk_size=st.session_state.chunk_size, chunk_overlap=st.session_state.chunk_overlap)
            st.session_state.embeddings  = embedding(openai_api_key=st.session_state.openai_api_key)

            # 3. ì‚¬ìš©ìê°€ ì„ íƒí•œ ë°©ì‹ì— ë”°ë¼ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
            if st.session_state.method == "ì„ë² ë”© ê¸°ë°˜":
                st.session_state.vectorstore = vectorstore(
                    st.session_state.embeddings, st.session_state.split_documents
                )
                st.session_state.retriever = retriever(st.session_state.vectorstore)
            else:  # ì‹œë©˜í‹± ì²­ì»¤ ê¸°ë°˜
                # ì‹œë©˜í‹± ì²­ì»¤ëŠ” ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—…ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê²°ê³¼ë¥¼ ìºì‹±í•´ì„œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
                if "semantic_chunks" not in st.session_state:
                    st.session_state.semantic_chunks = sementic_chunker(
                        st.session_state.embeddings, st.session_state.split_documents
                    )
                st.session_state.vector_store_instance = vectorstore_sementic(
                    st.session_state.embeddings, st.session_state.semantic_chunks
                )
                st.session_state.retriever = retriever(st.session_state.vector_store_instance)

            st.session_state.prompt = PromptTemplate.from_template(st.session_state.new_prompt)
            st.session_state.chain = create_models(st.session_state.retriever, st.session_state.prompt, st.session_state.openai_api_key)

            st.write(f"â¡ï¸ ì´ {len(st.session_state.pdf_files)}ê°œì˜ PDF íŒŒì¼ì—ì„œ {len(st.session_state.all_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            st.write(f"â¡ï¸ ë¶„í• ëœ ì²­í¬ì˜ ìˆ˜: {len(st.session_state.split_documents)}")
            if st.session_state.method == "ì‹œë©˜í‹± ì²­ì»¤ ê¸°ë°˜":
                st.write(f"â¡ï¸ ì‹œë©˜í‹± ì²­í¬ì˜ ìˆ˜: {len(st.session_state.semantic_chunks)}")
            st.text_area("â¡ï¸ í”„ë¡¬í”„íŠ¸ ë‚´ìš©", f"{st.session_state.prompt.template}", height=350)

    with col3:

        st.subheader("QA")

        st.markdown("---")

        st.session_state.question = st.text_area("ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!", "ë†êµ¬ì—ì„œ 3ì  ìŠ› ê¸°ì¤€ì— ëŒ€í•´ì„œ bullet points í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.", height=200)
        if st.button("ì‹¤í–‰", key="execute"):
                response = st.session_state.chain.invoke(st.session_state.question)

                st.markdown("---")

                st.subheader("Groundedness Check")

                # ë¶„í• ëœ ë¬¸ì„œ ë‚´ìš©ë„ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.
                relevant_docs = st.session_state.retriever.get_relevant_documents(st.session_state.question)
                pdf_contents = "\n\n".join([doc.page_content for doc in relevant_docs])

                # request_inputì— contextì™€ ìƒì„±ëœ ë‹µë³€(response)ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.
                request_input = {
                    "context": pdf_contents,
                    "answer": response,
                }

                st.session_state.groundedness_check = UpstageGroundednessCheck(
                    api_key=st.session_state.openai_api_key,
                    model="solar-pro",
                    temperature=0,
                    base_url="https://api.upstage.ai/v1",
                )

                # Groundedness Check ì‹¤í–‰
                st.session_state.gc_result  = st.session_state.groundedness_check.invoke(request_input)

                if st.session_state.gc_result.lower().startswith("grounded"):
                    st.text("âœ… Groundedness check passed")
                    st.markdown(response)
                else:
                    st.text("âŒ Groundedness check failed")
                    st.text("ğŸ” Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
                    st.session_state.tavily_retriever = TavilySearchAPIRetriever(k=3)
                    tavily_chain = create_models(
                        st.session_state.tavily_retriever,
                        st.session_state.prompt,
                        st.session_state.openai_api_key
                    )
                    # tavily_chainì„ í†µí•´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
                    tavily_response = tavily_chain.invoke(st.session_state.question)
                    st.text(tavily_response)